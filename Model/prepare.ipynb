{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.53.2)\n",
      "Collecting peft\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (1.8.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (3.6.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl (72.2 MB)\n",
      "   ---------------------------------------- 0.0/72.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/72.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/72.2 MB 1.7 MB/s eta 0:00:43\n",
      "    --------------------------------------- 1.0/72.2 MB 1.8 MB/s eta 0:00:40\n",
      "    --------------------------------------- 1.6/72.2 MB 2.0 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 2.1/72.2 MB 2.0 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 2.4/72.2 MB 1.9 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 2.9/72.2 MB 2.0 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 3.4/72.2 MB 2.0 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 3.9/72.2 MB 2.1 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 4.5/72.2 MB 2.1 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 5.0/72.2 MB 2.2 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 5.5/72.2 MB 2.3 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 6.3/72.2 MB 2.3 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 6.8/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 7.3/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 7.6/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 8.1/72.2 MB 2.3 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 8.4/72.2 MB 2.3 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 9.2/72.2 MB 2.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 10.0/72.2 MB 2.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 10.5/72.2 MB 2.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 11.3/72.2 MB 2.5 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 12.1/72.2 MB 2.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 12.6/72.2 MB 2.6 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 13.4/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 13.9/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 14.4/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 14.9/72.2 MB 2.6 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 15.5/72.2 MB 2.6 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 16.3/72.2 MB 2.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 17.0/72.2 MB 2.7 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 17.8/72.2 MB 2.7 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 18.6/72.2 MB 2.8 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 19.1/72.2 MB 2.7 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 19.7/72.2 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 20.2/72.2 MB 2.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 21.0/72.2 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 21.8/72.2 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 22.3/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 22.8/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 23.3/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 24.1/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 24.4/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 25.2/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 25.7/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 26.5/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 27.3/72.2 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 28.0/72.2 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 28.6/72.2 MB 2.9 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 29.6/72.2 MB 2.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 30.4/72.2 MB 2.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 31.5/72.2 MB 2.9 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 32.5/72.2 MB 3.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 33.3/72.2 MB 3.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 33.8/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.3/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 37.7/72.2 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 38.5/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 39.1/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 39.8/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 40.6/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 41.4/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 41.9/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 42.7/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 43.5/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 44.3/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 44.8/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 45.6/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 46.7/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 47.7/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 48.5/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 49.5/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 50.6/72.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 51.6/72.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 53.0/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 54.0/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 54.8/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 55.8/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 56.6/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 57.4/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 58.2/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 59.2/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 59.5/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 60.3/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.1/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 61.9/72.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 62.7/72.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 62.9/72.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 64.7/72.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 65.5/72.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 66.1/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 66.8/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 67.6/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 68.4/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 69.2/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 70.3/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.0/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.1/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.2/72.2 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes, peft\n",
      "\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   ---------------------------------------- 2/2 [peft]\n",
      "\n",
      "Successfully installed bitsandbytes-0.46.1 peft-0.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers peft accelerate datasets bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_15132\\787550989.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     50\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     51\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     52\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdefault_data_collator,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# 6. Train\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2207\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2549\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3750\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3858\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, default_data_collator\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 1. Load dataset từ file SQuAD json\n",
    "data_files = {\n",
    "    \"train\": \"../train.json\",\n",
    "    \"validation\": \"../valid.json\"\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# 2. Load tokenizer và model\n",
    "model_ckpt = \"deepset/xlm-roberta-base-squad2\"  # tốt cho tiếng Việt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "# 3. Tokenize function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# 4. TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'context', 'question', 'answers']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at goldfish-models/vie_latn_10mb and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"goldfish-models/vie_latn_10mb\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    \"goldfish-models/vie_latn_10mb\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    # Cần phải cung cấp tokenizer.pad_token_id cho AutoModelForQuestionAnswering\n",
    "    # nếu nó không tự xác định được.\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.53.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers->peft) (0.21.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): GPT2LMHeadModel(\n",
       "          (transformer): GPT2Model(\n",
       "            (wte): Embedding(38912, 512)\n",
       "            (wpe): Embedding(512, 512)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (h): ModuleList(\n",
       "              (0-3): 4 x GPT2Block(\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): GPT2Attention(\n",
       "                  (c_attn): lora.Linear(\n",
       "                    (base_layer): Conv1D(nf=1536, nx=512)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (c_proj): Conv1D(nf=512, nx=512)\n",
       "                  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): GPT2MLP(\n",
       "                  (c_fc): Conv1D(nf=2048, nx=512)\n",
       "                  (c_proj): Conv1D(nf=512, nx=2048)\n",
       "                  (act): GELUActivation()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (lm_head): Linear(in_features=512, out_features=38912, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up lora \n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "pert_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r = 8, \n",
    "    lora_alpha = 16, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, pert_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[38516,     9,  2762,    21, 38517]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38516,     9,  2762,    21, 38517,   288,  1715,   841,   176,   691,\n",
       "          122,     8,    85,   214,   704,   425,    25,    11,    18,   291,\n",
       "          209,    85,   214,    43,    46,     5,   549,   384,    10,   247,\n",
       "          160,    33,   704,   425,   309,   587,     4, 38517])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids = token[\"input_ids\"], \n",
    "    max_length = 100,\n",
    "    do_sample = True,\n",
    "    top_k = 50, \n",
    "    top_p = 0.95,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1\n",
    "    )\n",
    "\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Chào bạn[SEP] Chắc mắc bệnh viện cấp là điều trị bác sĩ sẽ được những phương pháp điều trị tại nhà và hỗ trợ các đơn vị về bác sĩ chuyên khoa.[SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi bị đau đầu và mệt mỏi, có phải là triệu chứng của Để điều trị bệnh nhân có thể được hiệu quả về tình trạng viêm da. Theo thông tin, việc sử dụng thuốc trong bài viết này để làm tăng sức khỏe cho trẻ em nên cần tìm thấy hiệu quả tốt nhất để bạn và các trường hợp của mình và cách chữa viêm xoang. Nếu bạn sẽ khiến nó với một số loại bỏ qua những bệnh tim ở những người bệnh tật để đạt hơn rất nhiều loại bỏ\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tôi bị đau đầu và mệt mỏi, có phải là triệu chứng của\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đào tạo nguồn nhân lực công nghệ thông tin chất lượng cao, góp phần tích cực vào sự phát triển của nền công nghiệp công nghệ thông tin Việt Nam, đồng thời tiến hành nghiên cứu khoa học và chuyển giao công nghệ thông tin tiên tiến, đặc biệt là hướng vào các ứng dụng nhằm góp phần đẩy mạnh sự nghiệp công nghiệp hóa, hiện đại hóa đất nước\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "\n",
    "context=\"\"\"\n",
    "Trường Đại học Công nghệ Thông tin (ĐH CNTT), Đại học Quốc gia Thành phố Hồ Chí Minh (ĐHQG-HCM) là trường đại học công lập đào tạo về công nghệ thông tin và truyền thông (CNTT&TT) được thành lập theo quyết định số 134/2006/QĐ-TTg ngày 08/06/2006 của Thủ tướng Chính phủ. Là trường thành viên của ĐHQG-HCM, trường ĐH CNTT có nhiệm vụ đào tạo nguồn nhân lực công nghệ thông tin chất lượng cao, góp phần tích cực vào sự phát triển của nền công nghiệp công nghệ thông tin Việt Nam, đồng thời tiến hành nghiên cứu khoa học và chuyển giao công nghệ thông tin tiên tiến, đặc biệt là hướng vào các ứng dụng nhằm góp phần đẩy mạnh sự nghiệp công nghiệp hóa, hiện đại hóa đất nước.\n",
    "Sau hơn 10 năm xây dựng và phát triển, hiện trường ĐH CNTT sở hữu cơ sở vật chất gồm khu học tập, nghiên cứu và làm việc được đầu tư xây dựng khang trang, hiện đại với tổng diện tích trên 14 hecta trong khuôn viên khu đô thị ĐHQG-HCM.\n",
    "\"\"\"\n",
    "question=\"\"\"\n",
    "Trường UIT mang trong mình nhiệm vụ gì?\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "try:\n",
    "  tokenizer.model_input_names.remove(\"token_type_ids\")\n",
    "except:\n",
    "  print(\"already removed!!!\")\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "print(tokenizer.decode(predict_answer_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bbc67b32aa4476aa049d26957269f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5b99b11a3e47e59f22c46329af092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a15e17744b04806b1bb7fe830c19faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "import torch\n",
    "dataset = load_dataset(\"parquet\", data_files={\"train\": \"../Data/Dataset/train-00000-of-00001.parquet\", \n",
    "                                               \"valid\": \"../Data/Dataset/validation-00000-of-00001.parquet\",\n",
    "                                               \"test\": \"../Data/Dataset/test-00000-of-00001.parquet\"})\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = f\"Answer the question: {example['question']} Context: {example['context']}\"\n",
    "    target = example[\"answer\"]\n",
    "    return {\"input_text\": prompt, \"target_text\": target}\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "\n",
    "def tokenize(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.1\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers==4.40.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (2025.7.9)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.0 MB 3.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 3.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/9.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.9/9.0 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.8/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/9.0 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/9.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: tokenizers\n",
      "\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "    Found existing installation: transformers 4.53.2\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "    Uninstalling transformers-4.53.2:\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "      Successfully uninstalled transformers-4.53.2\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   ---------------------------------------- 2/2 [transformers]\n",
      "\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Windows\\AppData\\Roaming\\Python\\Python310\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d765b334d214b3f9a2fbb8a257f148e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer_seq2seq because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_seq2seq.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:209\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.16.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[0;32m     19\u001b[0m     AutoPeftModel,\n\u001b[0;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[0;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[0;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[0;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[0;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[0;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\auto.py:32\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     PeftModel,\n\u001b[0;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[0;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[0;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[0;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[0;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\peft_model.py:37\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, HybridCache, PreTrainedModel\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vit5-qa-checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1500\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1512\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1513\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1515\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer_seq2seq because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./vit5-qa-checkpoint\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    eval_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
