{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.53.2)\n",
      "Collecting peft\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (1.8.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (3.6.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl (72.2 MB)\n",
      "   ---------------------------------------- 0.0/72.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/72.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/72.2 MB 1.7 MB/s eta 0:00:43\n",
      "    --------------------------------------- 1.0/72.2 MB 1.8 MB/s eta 0:00:40\n",
      "    --------------------------------------- 1.6/72.2 MB 2.0 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 2.1/72.2 MB 2.0 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 2.4/72.2 MB 1.9 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 2.9/72.2 MB 2.0 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 3.4/72.2 MB 2.0 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 3.9/72.2 MB 2.1 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 4.5/72.2 MB 2.1 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 5.0/72.2 MB 2.2 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 5.5/72.2 MB 2.3 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 6.3/72.2 MB 2.3 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 6.8/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 7.3/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 7.6/72.2 MB 2.4 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 8.1/72.2 MB 2.3 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 8.4/72.2 MB 2.3 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 9.2/72.2 MB 2.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 10.0/72.2 MB 2.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 10.5/72.2 MB 2.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 11.3/72.2 MB 2.5 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 12.1/72.2 MB 2.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 12.6/72.2 MB 2.6 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 13.4/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 13.9/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 14.4/72.2 MB 2.6 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 14.9/72.2 MB 2.6 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 15.5/72.2 MB 2.6 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 16.3/72.2 MB 2.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 17.0/72.2 MB 2.7 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 17.8/72.2 MB 2.7 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 18.6/72.2 MB 2.8 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 19.1/72.2 MB 2.7 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 19.7/72.2 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 20.2/72.2 MB 2.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 21.0/72.2 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 21.8/72.2 MB 2.8 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 22.3/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 22.8/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 23.3/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 24.1/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 24.4/72.2 MB 2.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 25.2/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 25.7/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 26.5/72.2 MB 2.8 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 27.3/72.2 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 28.0/72.2 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 28.6/72.2 MB 2.9 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 29.6/72.2 MB 2.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 30.4/72.2 MB 2.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 31.5/72.2 MB 2.9 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 32.5/72.2 MB 3.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 33.3/72.2 MB 3.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 33.8/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.3/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 34.9/72.2 MB 3.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 37.7/72.2 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 38.5/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 39.1/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 39.8/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 40.6/72.2 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 41.4/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 41.9/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 42.7/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 43.5/72.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 44.3/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 44.8/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 45.6/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 46.7/72.2 MB 3.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 47.7/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 48.5/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 49.5/72.2 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 50.6/72.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 51.6/72.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 53.0/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 54.0/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 54.8/72.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 55.8/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 56.6/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 57.4/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 58.2/72.2 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 59.2/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 59.5/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 60.3/72.2 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.1/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.3/72.2 MB 3.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 61.9/72.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 62.7/72.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 62.9/72.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 64.7/72.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 65.5/72.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 66.1/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 66.8/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 67.6/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 68.4/72.2 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 69.2/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 70.3/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.0/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  72.1/72.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.2/72.2 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes, peft\n",
      "\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   ---------------------------------------- 0/2 [bitsandbytes]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   -------------------- ------------------- 1/2 [peft]\n",
      "   ---------------------------------------- 2/2 [peft]\n",
      "\n",
      "Successfully installed bitsandbytes-0.46.1 peft-0.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers peft accelerate datasets bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_15132\\787550989.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     50\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     51\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     52\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdefault_data_collator,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# 6. Train\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2207\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2549\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3750\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3858\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, default_data_collator\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 1. Load dataset từ file SQuAD json\n",
    "data_files = {\n",
    "    \"train\": \"../train.json\",\n",
    "    \"validation\": \"../valid.json\"\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# 2. Load tokenizer và model\n",
    "model_ckpt = \"deepset/xlm-roberta-base-squad2\"  # tốt cho tiếng Việt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "# 3. Tokenize function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# 4. TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'context', 'question', 'answers']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at goldfish-models/vie_latn_10mb and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"goldfish-models/vie_latn_10mb\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    \"goldfish-models/vie_latn_10mb\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    # Cần phải cung cấp tokenizer.pad_token_id cho AutoModelForQuestionAnswering\n",
    "    # nếu nó không tự xác định được.\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.53.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers->peft) (0.21.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): GPT2LMHeadModel(\n",
       "          (transformer): GPT2Model(\n",
       "            (wte): Embedding(38912, 512)\n",
       "            (wpe): Embedding(512, 512)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (h): ModuleList(\n",
       "              (0-3): 4 x GPT2Block(\n",
       "                (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): GPT2Attention(\n",
       "                  (c_attn): lora.Linear(\n",
       "                    (base_layer): Conv1D(nf=1536, nx=512)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (c_proj): Conv1D(nf=512, nx=512)\n",
       "                  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): GPT2MLP(\n",
       "                  (c_fc): Conv1D(nf=2048, nx=512)\n",
       "                  (c_proj): Conv1D(nf=512, nx=2048)\n",
       "                  (act): GELUActivation()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (lm_head): Linear(in_features=512, out_features=38912, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up lora \n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "pert_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r = 8, \n",
    "    lora_alpha = 16, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, pert_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[38516,     9,  2762,    21, 38517]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38516,     9,  2762,    21, 38517,   288,  1715,   841,   176,   691,\n",
       "          122,     8,    85,   214,   704,   425,    25,    11,    18,   291,\n",
       "          209,    85,   214,    43,    46,     5,   549,   384,    10,   247,\n",
       "          160,    33,   704,   425,   309,   587,     4, 38517])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids = token[\"input_ids\"], \n",
    "    max_length = 100,\n",
    "    do_sample = True,\n",
    "    top_k = 50, \n",
    "    top_p = 0.95,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1\n",
    "    )\n",
    "\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Chào bạn[SEP] Chắc mắc bệnh viện cấp là điều trị bác sĩ sẽ được những phương pháp điều trị tại nhà và hỗ trợ các đơn vị về bác sĩ chuyên khoa.[SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi bị đau đầu và mệt mỏi, có phải là triệu chứng của Để điều trị bệnh nhân có thể được hiệu quả về tình trạng viêm da. Theo thông tin, việc sử dụng thuốc trong bài viết này để làm tăng sức khỏe cho trẻ em nên cần tìm thấy hiệu quả tốt nhất để bạn và các trường hợp của mình và cách chữa viêm xoang. Nếu bạn sẽ khiến nó với một số loại bỏ qua những bệnh tim ở những người bệnh tật để đạt hơn rất nhiều loại bỏ\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tôi bị đau đầu và mệt mỏi, có phải là triệu chứng của\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đào tạo nguồn nhân lực công nghệ thông tin chất lượng cao, góp phần tích cực vào sự phát triển của nền công nghiệp công nghệ thông tin Việt Nam, đồng thời tiến hành nghiên cứu khoa học và chuyển giao công nghệ thông tin tiên tiến, đặc biệt là hướng vào các ứng dụng nhằm góp phần đẩy mạnh sự nghiệp công nghiệp hóa, hiện đại hóa đất nước\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "\n",
    "context=\"\"\"\n",
    "Trường Đại học Công nghệ Thông tin (ĐH CNTT), Đại học Quốc gia Thành phố Hồ Chí Minh (ĐHQG-HCM) là trường đại học công lập đào tạo về công nghệ thông tin và truyền thông (CNTT&TT) được thành lập theo quyết định số 134/2006/QĐ-TTg ngày 08/06/2006 của Thủ tướng Chính phủ. Là trường thành viên của ĐHQG-HCM, trường ĐH CNTT có nhiệm vụ đào tạo nguồn nhân lực công nghệ thông tin chất lượng cao, góp phần tích cực vào sự phát triển của nền công nghiệp công nghệ thông tin Việt Nam, đồng thời tiến hành nghiên cứu khoa học và chuyển giao công nghệ thông tin tiên tiến, đặc biệt là hướng vào các ứng dụng nhằm góp phần đẩy mạnh sự nghiệp công nghiệp hóa, hiện đại hóa đất nước.\n",
    "Sau hơn 10 năm xây dựng và phát triển, hiện trường ĐH CNTT sở hữu cơ sở vật chất gồm khu học tập, nghiên cứu và làm việc được đầu tư xây dựng khang trang, hiện đại với tổng diện tích trên 14 hecta trong khuôn viên khu đô thị ĐHQG-HCM.\n",
    "\"\"\"\n",
    "question=\"\"\"\n",
    "Trường UIT mang trong mình nhiệm vụ gì?\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "try:\n",
    "  tokenizer.model_input_names.remove(\"token_type_ids\")\n",
    "except:\n",
    "  print(\"already removed!!!\")\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "print(tokenizer.decode(predict_answer_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bbc67b32aa4476aa049d26957269f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5b99b11a3e47e59f22c46329af092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a15e17744b04806b1bb7fe830c19faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "import torch\n",
    "dataset = load_dataset(\"parquet\", data_files={\"train\": \"../Data/Dataset/train-00000-of-00001.parquet\", \n",
    "                                               \"valid\": \"../Data/Dataset/validation-00000-of-00001.parquet\",\n",
    "                                               \"test\": \"../Data/Dataset/test-00000-of-00001.parquet\"})\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = f\"Answer the question: {example['question']} Context: {example['context']}\"\n",
    "    target = example[\"answer\"]\n",
    "    return {\"input_text\": prompt, \"target_text\": target}\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "\n",
    "def tokenize(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.1\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.40.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.40.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\windows\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers==4.40.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.40.1) (2025.7.9)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.0 MB 3.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 3.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/9.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.9/9.0 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.8/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/9.0 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/9.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: tokenizers\n",
      "\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "    Found existing installation: transformers 4.53.2\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "    Uninstalling transformers-4.53.2:\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "      Successfully uninstalled transformers-4.53.2\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   ---------------------------------------- 2/2 [transformers]\n",
      "\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Windows\\AppData\\Roaming\\Python\\Python310\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb94b0e90ef4dabb7097defa1d57ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe28b83f31424d53821800ca34bbfe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1630eeaba76949a2a64fb164a3a96071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b3d3fa220d4020bb929d44cd927a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8debb010062f43c38612a4369ad9f868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb46206c7864b7ea11c90c4c68d2b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "torch.set_num_threads(16) \n",
    "\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/vit5-fine-tuning-for-question-answering\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"../Data/Dataset/train-00000-of-00001.parquet\", \n",
    "    \"valid\": \"../Data/Dataset/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"../Data/Dataset/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = f\"Answer the question: {example['question']} Context: {example['context']}\"\n",
    "    target = example[\"answer\"]\n",
    "    return {\"input_text\": prompt, \"target_text\": target}\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=384\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./vit5-qa-checkpoint\",\n",
    "    eval_strategy=\"steps\",       \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    eval_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./vit5-qa-final\")\n",
    "tokenizer.save_pretrained(\"./vit5-qa-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "# định nghĩa các load model \n",
    "quantized_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_name = \"PhucDanh/vit5-fine-tuning-for-question-answering\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "                                              quantization_config = quantized_config, device_map = \"auto\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(36097, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(36097, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(36097, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=36097, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(36097, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(36097, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(36097, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear4bit(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=36097, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c386ec5b034183aee33d6b8fb20ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d311b944787f4d36baa45328c88693ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a58ae04d164acb93095cf97e3358ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4540b2745a594b07b8fe228b6acaf47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc29c84fd1904f628743e814f3fb22ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2be6e5a5dfb4a1ca32c2beaaa18ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995ee4d383a646eeaae320a46f86de4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc6414d010427990cfb99a890e41a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46831a556a42450388bfbbb2fc6b0749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"../Data/Dataset/train-00000-of-00001.parquet\", \n",
    "    \"valid\": \"../Data/Dataset/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"../Data/Dataset/test-00000-of-00001.parquet\"\n",
    "})\n",
    "\n",
    "\n",
    "def pre_processing(example):\n",
    "    input = \"Answer the question: {} Context: {}\".format(example['question'],example['context'])\n",
    "    return {\"input_text\": input, \"target_text\":example[\"answer\"]}\n",
    "\n",
    "dataset = dataset.map(pre_processing)\n",
    "\n",
    "\n",
    "def creat_token(example):\n",
    "    model_input = tokenizer(example[\"input_text\"], padding= \"max_length\", max_length= 384, truncation= True)\n",
    "    labels = tokenizer(example[\"target_text\"], padding= \"max_length\", max_length= 128, truncation= True)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = dataset.map(creat_token, batched=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir= \"./vit5-qa-checkpoint\",\n",
    "    eval_strategy = 'epoch', \n",
    "    save_total_limit = 1\n",
    "    save_steps =  2000,\n",
    "    eval_steps =  2000,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=  2,\n",
    "    eval_accumulation_steps=  2,\n",
    "    load_best_model_at_end=  True,\n",
    "    num_train_epochs = 4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "Trainer = Seq2SeqTrainer(model, data_collator = data_collator,TrainingArguments = arguments , train_dataset = tokenized_dataset[\"train\"], eval_dataset = tokenized_dataset[\"valid\"])\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./vit5-qa-final\")\n",
    "tokenizer.save_pretrained(\"./vit5-qa-final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
